import pandas as pd
import zipfile
import os
import numpy as np
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Step 1: Load the dataset
data_url = r"C:\Users\acer\.spyder-py3\used_car_dataset.csv"

# Check if the dataset is already extracted
if os.path.exists(data_url):
    df = pd.read_csv(data_url)
    print("Dataset loaded successfully.")
else:
    # Extract the dataset if it is zipped
    zip_file_path = r"C:\Users\HP\PycharmProjects\USED-CARS PRICE PREDICTION\used_car_dataset.zip"
    extract_path = r"C:\Users\HP\PycharmProjects\USED-CARS PRICE PREDICTION"

    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)

    df = pd.read_csv(data_url)
    print("Dataset extracted and loaded successfully.")

# Step 2: Data Preprocessing
# Check for missing values
print("Missing values in each column:")
print(df.isnull().sum())

# Clean 'car_price_in_rupees' column: remove currency symbols and convert to numeric
def clean_price(price_str):
    # Remove currency symbols and convert 'Lakh' and 'Crore' to numerical value
    price_str = price_str.replace('₹', '').replace(',', '').strip()
    if 'Crore' in price_str:
        return float(price_str.replace(' Crore', '')) * 10000000  # 1 Crore = 10,000,000
    elif 'Lakh' in price_str:
        return float(price_str.replace(' Lakh', '')) * 100000  # 1 Lakh = 100,000
    else:
        return float(price_str)  # Assume it's already a numeric value

df['car_price_in_rupees'] = df['car_price_in_rupees'].apply(clean_price)
print("Cleaned 'car_price_in_rupees' column.")

# Fill missing values for numerical columns with the mean
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
print("Filled missing values for numerical columns with the mean.")

# Fill missing categorical columns with the mode (most frequent value)
categorical_columns = df.select_dtypes(include=['object']).columns
for column in categorical_columns:
    mode_value = df[column].mode()[0]
    df[column] = df[column].fillna(mode_value)  # Avoid chained assignment warning

print("Filled missing values for categorical columns with the mode.")

# Remove duplicates
duplicates_before = df.shape[0]
df.drop_duplicates(inplace=True)
duplicates_after = df.shape[0]
print(f"Removed {duplicates_before - duplicates_after} duplicate rows.")

# Clean 'kms_driven' column: remove ' km' and convert to numeric
df['kms_driven'] = df['kms_driven'].str.replace(' km', '', regex=False).str.replace(',', '', regex=False).astype(float)
print("Cleaned 'kms_driven' column.")

# Handling outliers using Z-score for numerical columns only
df_numeric = df.select_dtypes(include=['float64', 'int64'])
z_scores = np.abs(stats.zscore(df_numeric))
df_cleaned = df[(z_scores < 3).all(axis=1)]  # Keep rows where z-scores are less than 3

print(f"Removed outliers. Remaining rows: {df_cleaned.shape[0]}")

# Step 3: Check and handle column names
df_cleaned.columns = df_cleaned.columns.str.strip()
print("Columns in df_cleaned:", df_cleaned.columns.tolist())

# Step 4: Encoding Categorical Variables
categorical_columns = ['fuel_type', 'city']  # Adjust based on your actual dataset

missing_columns = [col for col in categorical_columns if col not in df_cleaned.columns]
if missing_columns:
    print(f"Warning: Missing columns in the dataset: {missing_columns}")

df_cleaned = pd.get_dummies(df_cleaned, columns=[col for col in categorical_columns if col in df_cleaned.columns], drop_first=True)
print("Encoded categorical variables using one-hot encoding.")

# Step 5: Normalization and Scaling
scaler = StandardScaler()
numerical_features = ['kms_driven']  # Add more features if necessary.
df_cleaned[numerical_features] = scaler.fit_transform(df_cleaned[numerical_features])
print("Normalized and scaled numerical features.")

# Step 6: Feature Selection and Engineering
df_selected = df_cleaned[['year_of_manufacture', 'kms_driven'] + [col for col in df_cleaned.columns if col.startswith(('fuel_type_', 'city_'))] + ['car_price_in_rupees']]
df_selected['car_age'] = 2023 - df_selected['year_of_manufacture']
print("Created new feature 'car_age'.")

# Step 7: Data Splitting
X = df_selected.drop('car_price_in_rupees', axis=1)
y = df_selected['car_price_in_rupees']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Split data into training set ({X_train.shape[0]} samples) and testing set ({X_test.shape[0]} samples).")

# Step 8: Model Selection - Using Random Forest Regressor.
model = RandomForestRegressor(n_estimators=100, random_state=42)

param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
print("Best model found through GridSearchCV.")

# Step 9: Model Training.
best_model.fit(X_train, y_train)
print("Model trained successfully.")

# Step 10: Model Evaluation.
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Model Evaluation Metrics:")
print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("R² Score:", r2)

# Step 11: Visualize Actual vs Predicted Prices.
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Prices')
plt.show()

print("Visualized Actual vs Predicted Prices.")

# Step 12: Feature Importance Visualization.
importances = best_model.feature_importances_
features = X.columns

plt.figure(figsize=(8, 6))
plt.barh(features, importances, color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Feature Importance')
plt.show()

print("Visualized Feature Importance.")

# Step 13: Residual Plot.
residuals = y_test - y_pred

plt.figure(figsize=(8, 6))
plt.scatter(y_pred, residuals, alpha=0.7, color='green')
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('Predicted Prices')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

print("Visualized Residual Plot.")
